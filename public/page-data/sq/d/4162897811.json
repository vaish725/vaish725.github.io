{"data":{"featured":{"edges":[{"node":{"frontmatter":{"title":"Humanoid Robot Gait Generation via Imitation Learning","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABk0lEQVQoz32PzW7aQBSF/YB9kG4qdV/EIlLbTVUFpaJtGhYVIaT8eTwTA8YGgyuEsfHYEKMEXuarcKCKWrWLT+eM7rlnZgxhRZgy4qC/fXFeFpgyQYoR7VqJnhUj5BLzMDvl/sD41+CAaYUIldFpfad68Qph6adC+b9C63jjUZ/Ts0JslVGpvuRF+TXXzQVSRX/lnu8apy/2ZIR1FyMKDn6J6K8QXZ+P5U+ULtp8vWkgbY2pIoQ65lVc+BOG8lLsScpwmiJcjRxr+r6m46yw/RXdUcbVt1s+lN/izjfYfoIcp6hxivQ0tp9yN9EIN8FyNcYw2OAEG34MdKGR3tK0D8E1Sbbltr+l1q3w7qyE9PZ0nBR7+rQzmG2KR/Rna5xgzSy8x2jYKXWlmYU500VOa5BRacaYbkaa7xg4IfWrc97ULAZ+wCK6x3JX9JyY4VQz+ZnQG4bUZUzTTjBu7LQoenjcMZ7nXHZWqMmaTf7Ibrdjv9+jvC3vrz2q7RHjaUYwz7jsRJw3Ij63Er60E7x5TrDM+QWGmvn/gu10iwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/48f206bd3ea6e268e3d3f32f15a63cfa/3bd6c/humanoid_walk.png","srcSet":"/static/48f206bd3ea6e268e3d3f32f15a63cfa/9892c/humanoid_walk.png 175w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/b504f/humanoid_walk.png 350w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/3bd6c/humanoid_walk.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/48f206bd3ea6e268e3d3f32f15a63cfa/6c37f/humanoid_walk.avif 175w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/c6618/humanoid_walk.avif 350w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/b6890/humanoid_walk.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/48f206bd3ea6e268e3d3f32f15a63cfa/0e166/humanoid_walk.webp 175w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/674a1/humanoid_walk.webp 350w,\n/static/48f206bd3ea6e268e3d3f32f15a63cfa/de71e/humanoid_walk.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":287}}},"tech":["Python","PyBullet","Reinforcement Learning (PPO-IL)","Imitation Learning","Stable-Baselines3"],"github":"https://github.com/MayankD409/Humanoid_gait_generation.git","external":""},"html":"<p>Developed a pipeline to enable a humanoid robot to learn human-like walking gaits using imitation learning techniques. Leveraged the Human3.6M motion capture dataset, processed via motion retargeting, to serve as expert demonstrations within the PyBullet physics simulator. Implemented and trained Actor-Critic agents using Proximal Policy Optimization for Imitation Learning (PPO-IL), demonstrating successful motion imitation of stable walking behaviors.</p>"}},{"node":{"frontmatter":{"title":"Adaptive RL-MPC for Autonomous Lane-Changing","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABUUlEQVQY03WLy0sbURxGv9/rTuNkrA+kkJlEMJOb22BDSRtb6EIRX4SIM8RUXKmLgoYgqAGhq9JV6brQf7cMXXVRON/qOwfZUb417nRL3y18fup3rvPRU79x8Do77HQL78/8q71O68Rf/fCzX/7mp++dV2b71JdfPT69c42NGgAmAmh5Cc/3dn/Lux8AwJQmY3qY0fwLDft4vw0GQNXyTaCRSlEutfOkntDdTJ4WcnyC/f10Ph/Xk5iF2rmEntQTQRVy1uSHR14spNkkiOjaWpQktVrN0jTq9bQo6qur6yzrqi+iyDEbkTNzUWTOuTh2WeZarSiOFaoqYqLKooAB9nLFhkMbDORNX11kqmpVa2ZVrGrMxuJEBAwWEoJ8HPDnUnfeysaKTM/kciKjAzEShgiJ4B8USiCEixAugp+G8lu4/R0m30M+CemoojkOf9//8Qc5ODPL3rzq8QAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/3b7e9171ef6ee7d8b42eb75aab42039b/6ddef/sumo.png","srcSet":"/static/3b7e9171ef6ee7d8b42eb75aab42039b/9a7ce/sumo.png 175w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/5648b/sumo.png 350w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/6ddef/sumo.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/3b7e9171ef6ee7d8b42eb75aab42039b/7d59f/sumo.avif 175w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/cd24f/sumo.avif 350w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/aa528/sumo.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/3b7e9171ef6ee7d8b42eb75aab42039b/d3580/sumo.webp 175w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/4ec59/sumo.webp 350w,\n/static/3b7e9171ef6ee7d8b42eb75aab42039b/e5b43/sumo.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":237.99999999999997}}},"tech":["Python","SUMO","Reinforcement Learning (SAC/PPO/TD3)","Model Predictive Control","TraCI"],"github":"https://github.com/MayankD409/RL_MPC.git","external":""},"html":"<p>A hybrid RL-MPC framework for autonomous vehicle lane-changing, integrating Soft Actor-Critic/PPO/TD3 to dynamically adjust risk-aware weights based on traffic conditions.Designed and trained the RL agent using a curriculum-based protocol in the SUMO simulation environment, achieving 30% higher success rates, 25% lower collision rates, and 20% faster lane-change completion compared to traditional MPC systems.</p>"}},{"node":{"frontmatter":{"title":"Temporal Coherence Evaluation in Video-Language Models","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABQklEQVQY003K3UvCUBzG8f3fRVH4QhGpGVJR0FVeeBVRFF00yJchOksntErazrbjmWNn7pzthy9nM9KbPhcPz8VXYoylabparRjjGGPP8xjjnEchY2HIfEpjgE2QJMnmbBYAJAD40odqp6E05SiKLMto1eVeV2nVZa3fHX3raqfZVuo9VdE/tE992GrIbeW11ZCH2ru0WCxqtWo2s10qHAohBoO3/b2tyulxNrN7cV6WX57PKkcH+Z1yMVe9uX56vCsVcleX5ZNi/uH+VnIQwqaBHcedTCilhBBkIYwx9anv+wiZqtrt9VRN6+u6btv2z2hkIdNEJrKQFFK6ms+T5VIIkSRJxKezOFzMZ0KINE1dQgzDGK8RQpyxPZ5gzjkAeJ4n0SAAgOgfAIjXAMD1iGGPXM8NgiCgNORTHrM4/msopb95sTd7hN8gvwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/1a4f1a204ccef2861b9706854ac24d26/a2850/introduction.png","srcSet":"/static/1a4f1a204ccef2861b9706854ac24d26/e9f9e/introduction.png 175w,\n/static/1a4f1a204ccef2861b9706854ac24d26/6c4ed/introduction.png 350w,\n/static/1a4f1a204ccef2861b9706854ac24d26/a2850/introduction.png 700w,\n/static/1a4f1a204ccef2861b9706854ac24d26/b15d1/introduction.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/1a4f1a204ccef2861b9706854ac24d26/d2d56/introduction.avif 175w,\n/static/1a4f1a204ccef2861b9706854ac24d26/29dde/introduction.avif 350w,\n/static/1a4f1a204ccef2861b9706854ac24d26/df7d5/introduction.avif 700w,\n/static/1a4f1a204ccef2861b9706854ac24d26/627a9/introduction.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/1a4f1a204ccef2861b9706854ac24d26/cf95e/introduction.webp 175w,\n/static/1a4f1a204ccef2861b9706854ac24d26/df52e/introduction.webp 350w,\n/static/1a4f1a204ccef2861b9706854ac24d26/89855/introduction.webp 700w,\n/static/1a4f1a204ccef2861b9706854ac24d26/e0114/introduction.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":227}}},"tech":["Python","PyTorch","CLIP","BERT","HuggingFace"],"github":"https://github.com/MayankD409/Video-Temporal-Consistency-Analysis.git","external":""},"html":"<p>Developed a framework to evaluate temporal coherence in multimodal foundation models for video understanding tasks, introducing metrics like CLIPGain for temporal consistency in video captioning and leveraging BERTScore for semantic analysis in video QA. The project was tested on benchmarks such as TOMATO and MSR-VTT, providing nuanced insights into temporal reasoning. Open-sourced the methodology to guide advancements in time-aware AI research.</p>"}},{"node":{"frontmatter":{"title":"VisualOdom-Particle-Filter","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIEA//EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAGJdnCctJ//xAAYEAADAQEAAAAAAAAAAAAAAAAAAQIxE//aAAgBAQABBQK9UUzmyR5Mpr//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAEBAQAAAAAAAAAAAAAAAAAQAUH/2gAIAQEABj8CMIf/xAAcEAACAgIDAAAAAAAAAAAAAAAAAREhMWFBUYH/2gAIAQEAAT8hXrTLxKjb6Zi3m+RI3qSYtWf/2gAMAwEAAgADAAAAEGQP/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QR//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAQACAgIDAAAAAAAAAAAAAAEAESFhMUFRgeH/2gAIAQEAAT8QWzlQid/YK0PJaJqtWy4xfYgU15GdwvaXc//Z"},"images":{"fallback":{"src":"/static/4fc7272c45dc8b2d8a4d44381b0bb630/bddbf/drone_veckm.jpg","srcSet":"/static/4fc7272c45dc8b2d8a4d44381b0bb630/20376/drone_veckm.jpg 175w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/05972/drone_veckm.jpg 350w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/bddbf/drone_veckm.jpg 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/4fc7272c45dc8b2d8a4d44381b0bb630/dee2e/drone_veckm.avif 175w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/9c5ce/drone_veckm.avif 350w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/7b88a/drone_veckm.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/4fc7272c45dc8b2d8a4d44381b0bb630/a2e69/drone_veckm.webp 175w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/14999/drone_veckm.webp 350w,\n/static/4fc7272c45dc8b2d8a4d44381b0bb630/b9339/drone_veckm.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":404}}},"tech":["Python","ROS2","OpenCV","PyTorch","Particle Filter","CNN Encoders"],"github":"https://github.com/MayankD409/VisualOdom-Particle-Filter.git","external":""},"html":"<p>A vision-based localization and visual odometry system for drones utilizing particle filters and advanced image encoding techniques. This project integrates CNN and VecKM (Inner product sum) based encoders, and Histogram of Features encoders to accurately estimate similarity score to update the weights of particle filter and effeciently localize the drone.We were able to validate the performance through simulation in Gazebo and ROS2.</p>"}},{"node":{"frontmatter":{"title":"Structure from Motion","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAAClElEQVQoz3WLW2/ScBiH+yV2EhiTgbLhOMyWQllcYaPQ9URpgf432o5TYbSDcRiabGaLRo2OJeqFLkucN7vUMOOdbree4ocyTLehicmTX9739z4vNDx865KhIdfQeQ6WIwPzP0Bmc8BsCZrNgfFxzGrFLJag1YqNW4ImU2B0zD8y6h8b81+7hppMaN80B8wm1DoxZzl/gez2mNNJT05GHTfis14e9jCoj/P7OMTLYjCHB8XZGdrlJB0OwmYjJq4v2u2E3RGzTRI2WxSamuK9vmWvV5p2CQFUWcJVfkHlo3kqnJvHQGZJa2ZbRsbQUobMlEG8gKHZKZfgvMk5nQnI7Za9XsXnU2fc2dAdfUXYBGRepgpUOA9Pc3NuPkXktwudtlxvyfWO2qQXy26v6vHIHo8MIUgFhssw3E9/wEgL2/mEVuQ1MaKQPoq9TS74BI03mtmantE7Sj29ZMB+HUEqCFKBQqH2H7B2EGtR9I4mbeYTVTGSoxCRC6RoVErMl8qpjp6qbEgVla3NR7ZCWCsUakPh8O4lOL5DkA9Xwa7CVdk5hUfE1UQBLKhJFKTwXGPF2ABrmliPkY/DeN+HYrHuFUSXILtS+qmc1DP4ioSvFbj1dbZUiunleKkiGs2svp7WWPZJPL4fi3UhhjkY4DVJvWC5fSB0hGglyzyQqfsys5On71YjosI2amqnDBpJ/hHNvGKYA0gQji8RhWM++TbBH3L8AZ84TNN7Gea5zD0rc1tFqrEsvBTEN6n0UVI4Sib7PgRA729OAOhJoAekdwX6XpWudXhjA+xl+qf3AJxkpJPfDgA9SFVPr1BOB1dF/pRTP67KHxTlTFXPzsvP/f5Cg4rFb//ne6Hwo1j6ebF+KRa/Dgq/AAL1GaoqroKwAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/5aa436d26e6f5f143eef61db110f4e61/01a74/fountain_p11.png","srcSet":"/static/5aa436d26e6f5f143eef61db110f4e61/877f3/fountain_p11.png 175w,\n/static/5aa436d26e6f5f143eef61db110f4e61/17f29/fountain_p11.png 350w,\n/static/5aa436d26e6f5f143eef61db110f4e61/01a74/fountain_p11.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/5aa436d26e6f5f143eef61db110f4e61/4845b/fountain_p11.avif 175w,\n/static/5aa436d26e6f5f143eef61db110f4e61/6a610/fountain_p11.avif 350w,\n/static/5aa436d26e6f5f143eef61db110f4e61/11cdf/fountain_p11.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/5aa436d26e6f5f143eef61db110f4e61/10109/fountain_p11.webp 175w,\n/static/5aa436d26e6f5f143eef61db110f4e61/b5c5d/fountain_p11.webp 350w,\n/static/5aa436d26e6f5f143eef61db110f4e61/71014/fountain_p11.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":491}}},"tech":["Python","OpenCV","Open3D","NumPy","Matplotlib","SciPy"],"github":"https://github.com/MayankD409/Structure-From-Motion.git","external":""},"html":"<p>A robust Structure from Motion (SfM) pipeline implemented in Python, utilizing OpenCV and Open3D to reconstruct accurate 3D structures from sequential 2D images. This project processes image pairs to detect and match features, recover camera poses, perform triangulation, and visualize the resulting 3D point cloud, demonstrating effective 3D reconstruction and visualization capabilities.</p>"}},{"node":{"frontmatter":{"title":"MultiRobot Search and Rescue","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACxklEQVQoz1WRXU/bZhiGTZp+0BHWrxUqwVQ66IAiRYWmcxsTJ4FWE9VUddrUHrQFtHZoY6LNFrKksRWCJhhVxTY7iR3bAZyEryLtpD3ZP5i2H7Dj/ZJryiukqQe39OiW7uu9n+eVFEUhGo0SiURozYnxBLHxGJPJW9yeuM3ExATJZIJ4QiWqRIXUuEoykSR+I4ESVYgrcZFRVRWpUCiwvLxMejGN/avNVzszDP59iet7o8xr35JQkwxe+pirMyNcyQ0yNDpEeCTMwsIz7r69Q/9fvdyxP6VStlj/Zf1/YF7L0/SayGvXkF5JBFclguMBQsdDhMNXGR4bYVgZInI9wqnQaU5+2M6xlTYkW6Kz0I7pGFRKFaRsLieAmq7hVT203/L0zF8g+IPE+adnePbdc2obNTLpLEW9SNWpkppPcfmzfo5l2uhYOM6Xq5/jeR6GYSClUikWFxdJp9PYVZvGRoPJyUmOnA1wf+Y+db+OaRrYtovjOpRKJTY3Nnnw8AEd/e8xFhtle3Mbzz0EZjIZdF0n9yKHbdv49S2mZ6fpPt/N+qt10c51XVzHw3FcqtUqtVqNl2sv+ehiPzElRqPZEL5pmki5XI5isSiglmXhN33mnswx0DcgYC3Psi183xdzq2GlUmF3bxdZlrkp36TerItTCGA2m2VpaQlN00Rgf3Ofx9884vInA2I10e5QrWaO4wht+9vIURl5XOagcSA8wzQOgcUl8noe13J58scsPf+e4+TvR/jCvsd+Yx9vwxOPtaBm2WRna5fCjk73n6fp+ud9pvce4lbddxvqmo5ZMekqnSHYlDj6k8QptZO52TleH7wWv9i605a/hfajTl/yIkFLov1NgBOrAdYqP2OVrHdXbgXUgkLbc4mur89xJTxMZ0cnY9fGWFldoVwuMzU1RSgUorenlwuPPiDwvUTfi14Mx6BslvkPNzjsM6n1ZHEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/efd6764387ef6824e4fa513444d21b1a/d5bd7/MultiRobot.png","srcSet":"/static/efd6764387ef6824e4fa513444d21b1a/dfbac/MultiRobot.png 175w,\n/static/efd6764387ef6824e4fa513444d21b1a/76a13/MultiRobot.png 350w,\n/static/efd6764387ef6824e4fa513444d21b1a/d5bd7/MultiRobot.png 700w,\n/static/efd6764387ef6824e4fa513444d21b1a/d7af7/MultiRobot.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/efd6764387ef6824e4fa513444d21b1a/55e84/MultiRobot.avif 175w,\n/static/efd6764387ef6824e4fa513444d21b1a/dee16/MultiRobot.avif 350w,\n/static/efd6764387ef6824e4fa513444d21b1a/29250/MultiRobot.avif 700w,\n/static/efd6764387ef6824e4fa513444d21b1a/1defd/MultiRobot.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/efd6764387ef6824e4fa513444d21b1a/1ab83/MultiRobot.webp 175w,\n/static/efd6764387ef6824e4fa513444d21b1a/5466f/MultiRobot.webp 350w,\n/static/efd6764387ef6824e4fa513444d21b1a/1bd4a/MultiRobot.webp 700w,\n/static/efd6764387ef6824e4fa513444d21b1a/51088/MultiRobot.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":376}}},"tech":["C++","ROS2","OpenCV","GoogleTest","Doxygen","YOLOv5"],"github":"https://github.com/MayankD409/MultiRobot_Search_and_Rescue.git","external":""},"html":"<p>A swarm robotic system for Search and Rescue operations, developed with C++ and ROS2. It features autonomous navigation, real-time object detection using YOLOv5, and seamless multi-robot coordination. The system supports dynamic robot spawning via configuration files or in-code settings, integrates with Nav2 for advanced path planning, and utilizes Gazebo for realistic simulations.</p>"}},{"node":{"frontmatter":{"title":"Right Invariant Extended Kalman Filter (RIEKF) for Object-Based SLAM","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB80lEQVQY0wHoARf+ANWurqOSkmtqbYKFidvW2NPLzdzX3uLY4t3W383LzcjDyNDKz97Oz9jN0d3b3Ofk5dDM16Ohn7awodrY3QDUjJJqUjwkCCNIKlLFvL7Yu7/arqi9qLCZl6mhnZq2sMKTd6OvhJC8sbPKxMWyqrqtmauonnaTjX3KwcUA1o6NoXV3fnmLsJmmxrex0qydyKCMt7empZyn2tfQ6+LqmJGtwKuR08/Dw7zAfG1/ooVKqoE+tHWV0cPGAMiXkbCSkImMeZealL6yr7mhlcG3rc/Hwcu6v9TRz+DV1crI0uvh3+rd4L67vaKZnI6KiXRuboxzd8S+wwDQi49yXUwhESE8LUnJwcTTv8bZwMLLt8Wxrb2gnJyup7Wqj67DoK3DtLrNyMnTzda4rMuZmYiSlXzIw8gA25aZjmtjZUlylGWQx72527ixzZF5s6WgmZalxcG83NbijXqdr419wr20ycDDh3yRr5FswJtTrn6T0cXHAMmGgKZ9f4CMeaOpnb+tqL2bir2pmr+9rriord7a1Orc3rOyyN3Sxurg3ry3u4N0fXxuV2xZSphtgMm+wgDf3NrLyMXPxsbf293l5eTPy8nAurnPx8jg3t3t7O3j4uHPzM3V09fn5efo5+fT09O2t72ztrzIzcro6ektdkem1AdrkQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ed7d9546bc7595cc4a0c07c42758e28/4215f/image.png","srcSet":"/static/7ed7d9546bc7595cc4a0c07c42758e28/ac100/image.png 175w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/c98a6/image.png 350w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/4215f/image.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/7ed7d9546bc7595cc4a0c07c42758e28/e2028/image.avif 175w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/62975/image.avif 350w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/4c07d/image.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/7ed7d9546bc7595cc4a0c07c42758e28/2e40f/image.webp 175w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/16972/image.webp 350w,\n/static/7ed7d9546bc7595cc4a0c07c42758e28/31199/image.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":269}}},"tech":["Python","NumPy","SciPy","Matplotlib"],"github":"https://github.com/MayankD409/RIEKF_Python.git","external":""},"html":"<p>A classic Python implementation of the Right Invariant Extended Kalman Filter (RIEKF) based on the seminal paper \"A Right Invariant Extended Kalman Filter for Object-Based SLAM.\" This project provides a robust framework for simultaneous localization and mapping (SLAM) using object-based observations. I have tried to do detailed documentation and keep a modular code structure to facilitate easy integration in practical applications.</p>"}},{"node":{"frontmatter":{"title":"Quadruped Gait Simulation","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB40lEQVQoz22TzY+aUBTF+W8cBOQ9tCkWMeM3fjSZmDh10UU3TdNFN131z+ym6UIzMtNgCWpt/EQTc5p7LQ6dDsnJgwf87jn3gvL123eM73xE0QyLxQKr1Qr3vo/hcAjHcVCtVlGpVHgl1Wo1VrKfqFQqYTAYQPnxM0IQRkgf8/kcrVYLV1cqhBAwTTMl8d8eXWezWTQaDSj+5A6+7+NwOGC3212AvV4PhmFAFgqQlgVLSlhWHkKYMHQDUkgIaUFKiXw+j1wuB8/zoEynAcbjMWazGcIwZCCddzodaJoGRwjkhWAoveRWrvH+42fc3N7CfmWzO8uyoOs6p1KiaI5fy9/sjlw+BbqmiaamsUNd19Du3ODDpy948/YdKtcNLvIPcBrOsN3HOB6PrATYbrdhqOo5qpQcjdy8tItwHBee95pdUw8lF/sLDIIAcRxjs9nwhNNALZvl6qY4D+I8jBwKhRcMIne0R0Dqd7PZhPLwcI+J72O5XDI0AXa7XY5MD6enmTh9LPDE4Wg0wmQywXq9wX6/Z2AURZceksPnQEKc25DosYdBgO12y1qv1xyfPnC6mclkLu4o0nOi2CRVVVGv16HQZC+KY5xOJ45MX71t2yiXy/wXuK7La1rJHq3FYhH9fh9/AMeJ+NuoQSamAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/9f735d5dcd972d6f36346ea58ee3f844/1dc65/bruno_rviz.png","srcSet":"/static/9f735d5dcd972d6f36346ea58ee3f844/9a130/bruno_rviz.png 175w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/47c72/bruno_rviz.png 350w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/1dc65/bruno_rviz.png 700w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/6f7a3/bruno_rviz.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/9f735d5dcd972d6f36346ea58ee3f844/dae43/bruno_rviz.avif 175w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/69c10/bruno_rviz.avif 350w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/ebe22/bruno_rviz.avif 700w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/bc1a4/bruno_rviz.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/9f735d5dcd972d6f36346ea58ee3f844/5d873/bruno_rviz.webp 175w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/853c6/bruno_rviz.webp 350w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/978b5/bruno_rviz.webp 700w,\n/static/9f735d5dcd972d6f36346ea58ee3f844/b7d6c/bruno_rviz.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":392}}},"tech":["C++","ROS2","Gazebo","Python3","URDF","Solidworks"],"github":"https://github.com/MayankD409/Bruno_Quadruped_Sim.git","external":""},"html":"<p>A simulation of a quadruped robot developed using C++ and ROS2 Humble. This project includes a detailed URDF model exported from SolidWorks, ROS2 packages for simulation and teleoperation. It enables autonomous navigation and real-time control through Joypad commands, offering a flexible environment for testing and refining quadruped gaits. Currently, the implementation supports trotting and crawling gaits, with plans to incorporate additional gait patterns in the future.</p>"}}]}}}