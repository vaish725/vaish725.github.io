{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/blog/multi-agent-rl-traffic-management",
    "result": {"data":{"markdownRemark":{"html":"<p><em>As our cities grow and traffic congestion becomes an everyday hurdle, the quest for efficient traffic management solutions has never been more crucial. Imagine a world where traffic lights and vehicles communicate and adapt in real-time to ease congestion and reduce travel times. is stepping in to revolutionize autonomous traffic management, paving the way for smarter and more responsive urban mobility systems.</em></p>\n<h1><strong>Introduction</strong></h1>\n<p>Autonomous driving systems are grabbing tons of attention and millions in funding from Industry, academia in recent years. The goal of such systems is to develop an efficient transportation system capable of replacing human drivers in order to reduce workload and improve safety and economy as a whole. We have already started to see commercial institutions like Waymo, Cruise, Zoox, Tesla etc. deploying their fully autonomous vehicle fleet on the roads of cities like San Francisco, Las Vegas, Dallas etc. and it's not long before we start to encounter these system in our daily commute all over the world.</p>\n<p>Even though the present system can be termed as operationally safe they are far from being perfect and there have been Multiple cases of mishaps recorded in the past involving these cars. Improving traffic control is still a very active area of research and thanks to the advancement in Multi-Agent reinforcement Learning (MARL) we have achieved substantial breakthroughs in the area. MARL enables multiple sub-systems involved in traffic control to efficiently communicate with each other creating a robust solution to the problem.</p>\n<h1>Problem Statement</h1>\n<p>Before moving ahead, it is important to understand what problem we are trying to solve. Autonomous driving is a complex task that requires a vehicle to navigate through a dynamic environment, avoiding obstacles and other vehicles while reaching its destination safely and efficiently. The vehicle must perceive its surroundings, predict the behavior of other agents, and make decisions in real-time to achieve its goals.</p>\n<p>In the context of traffic management, the complexity further intensifies when we consider multiple autonomous vehicles, pedestrians, and traffic control elements all interacting simultaneously. Traditional traffic control systems rely on static rules and timed signal patterns, which often fail to adapt efficiently to unpredictable changes in traffic flow. This rigidity can lead to increased congestion, longer travel times, and higher emissions. The challenge, therefore, is to develop a dynamic and adaptive solution that allows all agents — vehicles, pedestrians, and traffic signals — to seamlessly coordinate with each other. By leveraging Multi-Agent Reinforcement Learning (MARL), we aim to create an intelligent system where each component learns and evolves in response to the environment, leading to improved overall traffic efficiency and safety in real time. This decentralized yet cooperative approach allows the system to handle the inherent uncertainties of urban traffic while optimizing flow and minimizing conflicts.</p>\n<h1>Fundamentals of Multi-Agent Reinforcement Learning</h1>\n<p>Multi-Agent Reinforcement Learning (MARL) extends the concepts of traditional RL to environments with multiple agents. Each agent learns to make decisions based on its individual observations while considering interactions with other agents. MARL is typically formulated as a Markov Game, a multi-agent extension of the Markov Decision Process (MDP). In this setting, each agent has a policy that governs its actions, and the agents collectively influence the state of the environment. MARL is a branch of reinforcement learning that deals with environments where multiple agents operate simultaneously. Unlike traditional RL, where a single agent interacts with a static environment, MARL involves multiple agents that interact with both the environment and each other, creating more complex dynamics.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/de83351ccb28ca118b2a5dd954868cd9/eefce/multi-agent-system.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.42857142857143%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsSAAALEgHS3X78AAACNUlEQVQoz1WT61ajMBSFef8HG+sqS5dQbAWBCoTcc5JyMyUtMxKn1u9XVtj7nJNNEhhjttttlmXLb67X6209jmMYhmmaOufuNUHTNM/Pz1mWaa3hP1JKr/MlGGNhGGZZppQCAKWU1poxFnjdx8dHnuc3tVIqyzKE0PF4LMsSITRNE8Y4TdPryrIsUsoAAKZpyvM8SZK+76dpcs4RQg6HgzEGANq2TdN0GIamaV5eXvq+tysAEBBCAKAsy/1+fzgcdiuUUmvtbRBKKWOsqiqvSZIkjuO6rgOt9TiOzjkhRRzF0zTN86y1vlwut2CMMcMwOOe01mEYes1X5/P5PM/zOI6U0qIohBDLsnx+ft6nOq+M4yiE2O/3WmuvCS6Xi7WWUlrXdVEUjDFrrXPueofXMMYQQq+vr1prrwmWZfEBCCHKsuSc3w9839xaa4xJkqTrunmev/7zsizn8xkh9PT0lKZpXdeMMc65EIIx1rbtbU0IiaIojmOEUNd13+Z5nt/e3vjK8XhUK4yxpmmUUnVd+7tRVRXGGADe39+10T+dbwpKKQBgjKWUlNKu6yilnHNKqRCCc26MadvWnE7fZmstIcQP5p1d1yGEpJQIIUII59zXFUJ81VXSB/5jPp1OWmulFMZ4GAYhxDAM0zT5TQAghJgVIX+b27b178EfOM/zKIp2u10cx1EUFUVBKfWfQCkuBAD8BIYxZit+tqqqNpvNdrt93Dw+/HkoikIp5S+pP13f9//MfwGqgHwif3somQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Schematic of Multi-agent System\"\n        title=\"Schematic of Multi-agent System\"\n        src=\"/static/de83351ccb28ca118b2a5dd954868cd9/39600/multi-agent-system.png\"\n        srcset=\"/static/de83351ccb28ca118b2a5dd954868cd9/1aaec/multi-agent-system.png 175w,\n/static/de83351ccb28ca118b2a5dd954868cd9/98287/multi-agent-system.png 350w,\n/static/de83351ccb28ca118b2a5dd954868cd9/39600/multi-agent-system.png 700w,\n/static/de83351ccb28ca118b2a5dd954868cd9/eefce/multi-agent-system.png 720w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>Schematic of Multi-agent System. It consists of an environment and multiple decision-making agents (shown as circles inside the environment). The agents can observe information about the environment and take actions to achieve their goals.</em></p>\n<h2>Markov Game (Stochastic Game)</h2>\n<p>MARL is often modeled as a Markov Game, an extension of the Markov Decision Process (MDP) for multi-agent systems. Each agent (i) has its own policy (πi), which maps states to actions. The environment's state is influenced by the actions of all agents, and each agent receives individual rewards based on the joint action of all agents and the resulting state. This scenario becomes increasingly complicated when we are dealing with limited or incomplete information about the overall system. More particularly, an agent may not even be aware of the presence of other agents, making the environment non-stationary. Even in the case of complete information, learning a fully joint state-action space can be computational intractable with respect to the number of agents and the level of coordination required between them. There are many categorization schemes of Markov Games: independent learning versus joint action learning, general Markov games versus normal form games, continuous action space versus discrete action space etc.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2f6b8e73476414b8124c6fe67e4a6efd/39600/mdp-vs-markov-game.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34.85714285714286%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsSAAALEgHS3X78AAABS0lEQVQY032QzUrDQBRG85ziAwi67MJuhIJPIuhGEEGlKoqIm1qogopTJjYVJyaNSTqTmemdm5/JSNudC8/+wPk+z/2LtbaqKmMMIgohCCFKKQDgnNd17bUr6rpGRKVUVVW4Yi0jYp7nUkrOeRRFjIX5fC6EkFJmWeYBACICgDHmMwjKskRErbUxxjkHAEmSKKWklEKIyYdfFEJrPZvN4jj2yrJc5jXN6On58fl1MmVaa+fcWjbGMMb0YmGtHQyHZ1c3r++0EEXT1FEUeWmaIcD9YLi5tbPb7XT2eg+DUdPUALDOJoTweT56edvY2t7tdrq9/f7tA2NfjIXLbNs03z/p0cXJef+gf3eaFnLZ0rbr2ZzzLE1naXZ8dXZxfXh5dxpm2UJrRPSstb7vjwmJYzam4yCYUkrDMPxze57nPiWTaUApTZPEOde27S+4C3x/B/oN4AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MDP vs Markov Game\"\n        title=\"MDP vs Markov Game\"\n        src=\"/static/2f6b8e73476414b8124c6fe67e4a6efd/39600/mdp-vs-markov-game.png\"\n        srcset=\"/static/2f6b8e73476414b8124c6fe67e4a6efd/1aaec/mdp-vs-markov-game.png 175w,\n/static/2f6b8e73476414b8124c6fe67e4a6efd/98287/mdp-vs-markov-game.png 350w,\n/static/2f6b8e73476414b8124c6fe67e4a6efd/39600/mdp-vs-markov-game.png 700w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>Illustration of the difference between a Markov Decision Process and a Markov Game. In (a) one agent and one environment are seen. The agent interacts with the environment by sending a tubule of actions and receiving one reward and the states of the environment. In (b) multiple agents and one environment are seen. The action space is split into i number of actions. Each agent receives a reward and the states of the environment.</em></p>\n<h2>Joint Policy and Interaction</h2>\n<p>In MARL, each agent needs to account for the behaviors of other agents, which makes the learning process interdependent. A joint policy π = (π1, π2, …, πn) defines the action-selection strategy for all agents. Agents must strike a balance between cooperation (if their goals align) and competition (if their goals conflict).</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4f553cc2830e39b42f08dd3d61b0b0ce/39600/agent-interaction.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAAExUlEQVQ4yyXK2U+aBwAA8O/P2Mv2sKRLlizrDpNVe04rcrQKiCKiiIBUqWgPsR49tqm1rZ11ztbWeVPAIqicKsghyCGX5RAKtRxagY/v4wNn0vWlybLs5ff0Aw5TSRBMp9PJVCqZyyGJ/X2tVuvz+91ul9PtdLldPr/X5/MKhcJoNJrPI2AGBME0CKZAMAWkMiAMQ5ksnDvKxxMxlXLl13t3mAz64OB9kUgoES8IF0RsJp3DoGt1uigMIzkEgiEQgsBMBoBySDILwcmEzbKpXVPodWtGs0m8KGGxmhrq6+rrqCwaVbog2NgyV08Ih7Tm/cju4eE+nIVyeQRQW1xElW/UFV6WLiqXxOsajV694LWqXToJkVRVWoG1z0x+1K5uGY2PZ/lmo35pUSATC1akEqfLCfT2dH+Gonx1f46v1mi12g3lqwPLeHx7Zi+grmJVF9MuhmSy6PNn4ItnaYV0y6R/8niogULGE/DXuRzAG9hZG+8j4HBEDKqDQ18WzKRci8GI1RwKtDVeYleeszmcKb0uqV6TC+fvdfGwOBytpkb4x/COxwGI+PNZ/3LEvzU23HsD/30R+sx13nWxSDQ9O1XPYpTTCL+Pj8rnXjLRKELhqVsdvIknD0IK/iedyiBeADwbL5Z2JU/cyvl3Js/mZu/QnRJcKZGIpqJOUPHlNSTsmfJT2OpLmPPn797utchlcY/yH/NfUdmIXToFrPqUKo/16sycbl0Ox8JLkuVizAUqpYBOLMQWF5TVlmDJ+AoaqgB9dpI/EQwGpDa1zTKjfdm3Mj8E1FRVdbNaWDh0UzWZ08zGUKoIxNK2kh/Ki8913uKRmbU4GoqEx17GXqipPUsh/UhkY05TfyZXlKBLigFmR9e1/oGrg8M3B4cGeDfaunp+uIimlX1HIJ2mXGUXUZiFDczKutKas9/c4zJG79x81Nf+9CGv7xfutc4WQLCP8FPIXCKjSeWPwbRw4k9uPbqMeL6MeoGOOkmrv/gT6dJJWnM5reHhyIN0NKaPmxdjG9NxterADEwtqxVb9tVkdtNm5DbXYQpPENDn+rq7cK03zxAuV1d+S2JQejgtdMYVVCOBzKg12PXumGPFJhrTTQERg2HH5ZmeHOnGnSC3Ejm9ba+mRlLBnUgwiKml4nAop8cJRsISubi9pZVQjGpnsHr6+j12hzVhA4YfPqi9PfD1qaLThQWToom0V/EppkeCq2BIz+U0s5uu2ELburh9JWEQ+FdFgllGDaUIVVbC63n0fAxo7+hs5bbe7epcNxpNiQQY1uh81uTOCuRXNlLwFQSi1K1cequdDStMb3SHIY9SLpt+Pk6qrKQ3MoH3Gdiy7dgN7SqXxVazYTOe1sjnFc9umAP2piZ2Q2OjwqS2uyzpN66Adn1MohCuacybBqvdkcxkgAyCHH/48DYeP4jFRCp116MRp9VgOkz0jz798ovP+wd+2w0EIx7Dgcco1ZsGVRt7B/vvk+/zx8dIPg+kIQjKwkg+hxwdhQ+T0YMDwUs+t5V7hd5wt7fb5bCtrqmsVotMpX7t2v54fATDcDaHQFk4k4UBMJuFsjCUzcIIks/n8n8f7b3bE4hE247tTAZ02K1vI2Gv93UKTMM5JIPkYOS//7//Ahf/76iFppo3AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Agent Interaction\"\n        title=\"Agent Interaction\"\n        src=\"/static/4f553cc2830e39b42f08dd3d61b0b0ce/39600/agent-interaction.png\"\n        srcset=\"/static/4f553cc2830e39b42f08dd3d61b0b0ce/1aaec/agent-interaction.png 175w,\n/static/4f553cc2830e39b42f08dd3d61b0b0ce/98287/agent-interaction.png 350w,\n/static/4f553cc2830e39b42f08dd3d61b0b0ce/39600/agent-interaction.png 700w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>A visualization for the interaction between agents in a multi-agent reinforcement learning environment with a focus on cooperation and competition at intersections.</em></p>\n<h1>Exploration &#x26; Exploitation</h1>\n<p>As in single-agent RL, each agent in MARL must explore the environment to discover better policies, but this becomes more challenging because other agents are also learning and adapting simultaneously. This dynamic changes the nature of the environment, making it non-stationary from any one agent's perspective.</p>\n<h2>Types of MARL</h2>\n<ol>\n<li><strong>Cooperative MARL</strong>: Agents share a common goal, such as maximizing the overall efficiency of a traffic network.</li>\n<li><strong>Competitive MARL</strong>: Agents have competing goals, such as individual vehicles aiming to minimize their own travel time, even at the expense of others.</li>\n<li><strong>Mixed MARL</strong>: Agents cooperate on some aspects but compete on others, such as vehicles coordinating at an intersection but competing on highway lane selection.</li>\n</ol>\n<h1>MARL for Traffic Management</h1>\n<p>Now that the fundamentals of MARL have been established, lets look at how we can utilise it for the purpose of traffic control. Like mentioned earlier traffic management involves multiple subsystems like vehicles, traffic lights, pedestrians, signposts etc. making this an extremely non-linear problem. Sub-optimal decisions of any sort can easily have disastrous results and hence we cannot rely on heuristics based control approaches.</p>\n<p>MARL is highly applicable to autonomous traffic management because the traffic environment involves multiple interacting agents, such as vehicles, pedestrians, and traffic lights. Each agent has its own goal (e.g., minimizing travel time), but they must also coordinate their actions to ensure the smooth functioning of the entire system.</p>\n<p>There are multiple ways we could approach this problem and there have been multiple attempts involving MADDPG with recurrent actor critic, MA3C, PRIMAL etc. but two of the most popular algorithms are Multi-Agent Advantage Actor-Critic (MA2C) and Multi-Agent Proximal Policy optimization.</p>\n<p>The <strong>Multi-Agent Advantage Actor-Critic (MA2C)</strong> algorithm adapts the traditional actor-critic reinforcement learning framework to environments with multiple interacting agents. Each agent — whether a traffic light or a vehicle — functions with its own \"actor\" and \"critic.\" The actor decides on the best action to take (like adjusting signal timing or changing lanes) based on current observations, while the critic evaluates the action by estimating an \"advantage\" function, which reflects the expected improvement over the average outcome. Fig. below shows the MA2C algorithm for traffic control.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 644px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/970cb5f81a0e426edc61e3f06f98420f/1ee40/ma2c-algorithm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAIAAABPIytRAAAACXBIWXMAAAsSAAALEgHS3X78AAABhElEQVQ4y5WUy47dIBBE/f/fFs0uydXMGL9tbAONeTOAia43UaQk19SKzVF3VXdTcQ77vu37ppRyzpkb0lpba3POVd+34zSM04AxjiHkAp3VNI+f9QdCdd93ztkSOFeUkr7vxnmSSqaUyuB1X4012pgYYzG8LPPH5/s4jQCMHxDCVwFsrSZ0X/AyjD1lhDLqvbsLSyWVUk2LMF4WPCuj7zdfCSGM0W3bSPV8xFgwrYof4LzTWm0bpoxYZ9N5u3KN6m3fhBAhBO99WdrGaKlU27Vd1zKgz8U5z/uej2HsUVM754rnDBwejx9vb98oJVKK7dqZlGII4XxlvlJaaa2naWhbRBnhBwghnLfO2ZTiy7R5CGGax6ZBlJL7hp8woXvOmXPAeMF4+fIl6ymVyDkfgjNgO9mA85fd/oZRg7Qx1hp2iVDinLsuLF4K6dI/rmpdYowYz+uKGbDjOKQQACClOAQHDlJJY/TfYa0V5+zn43ssHPIT9t4DMLzilNJ56Y9v6r/h/wLZBQdlDUCUSQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MA2C Algorithm\"\n        title=\"MA2C Algorithm\"\n        src=\"/static/970cb5f81a0e426edc61e3f06f98420f/1ee40/ma2c-algorithm.png\"\n        srcset=\"/static/970cb5f81a0e426edc61e3f06f98420f/1aaec/ma2c-algorithm.png 175w,\n/static/970cb5f81a0e426edc61e3f06f98420f/98287/ma2c-algorithm.png 350w,\n/static/970cb5f81a0e426edc61e3f06f98420f/1ee40/ma2c-algorithm.png 644w\"\n        sizes=\"(max-width: 644px) 100vw, 644px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>MA2C Algorithm for MARL approach</em></p>\n<p>Initially, we set up parameters, states and policies for all agents. Each agent observes its local environment and uses a policy network to decide on the next action — such as changing a traffic light or adjusting speed — based on current observations and past experiences. Actions are taken to interact with the environment, and agents receive rewards that reflect the immediate impact on traffic flow, like reduced congestion or wait times. These experiences are stored in a memory buffer. Periodically, agents use the collected experiences to update their policy and value networks through the Advantage Actor-Critic (A2C) method, which optimizes their strategies by balancing exploration and exploitation. By continuously updating their policies and learning from the environment, the agents collaboratively improve traffic conditions over time without centralized control.</p>\n<p>Another prominent algorithm employed in traffic control is the <strong>Multi-Agent Proximal Policy Optimization (MAPPO)</strong>. Similar to MA2C, MAPPO adapts a single-agent reinforcement learning algorithm — in this case, Proximal Policy Optimization (PPO) — to a multi-agent context. In traffic management scenarios, each agent, such as a traffic light or a vehicle, utilizes MAPPO to learn an optimal policy that not only benefits its individual performance but also contributes to the overall efficiency of the traffic network.</p>\n<p>MAPPO operates by allowing each agent to interact with its local environment and make decisions based on observations like traffic density, queue lengths, or signal states. The key distinction of MAPPO lies in its use of a clipped surrogate objective, which constrains policy updates to a proximal region. This approach prevents drastic changes in the policy during training, enhancing the stability and reliability of learning. Agents collect experience data over multiple time steps, storing them in a memory buffer. They then perform policy updates using this batch of experiences, optimizing for cumulative rewards that reflect improvements in traffic flow, such as reduced waiting times and smoother vehicle movements.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fa3947ed0bdef594d561c880648530c0/39600/mappo-representation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.71428571428571%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsSAAALEgHS3X78AAABf0lEQVQoz3WRO5LqMBBFvW1WwSLIKCJ2QEgGbzJcxqYkWQLLEt36WrbxFKjm1SRzgpt1dd/ThfdeKaW1Vh/MB2st5/z0gTEGAPrD8wettbW2UEqllGKMKSVjTFmW1+u1aRrnXN/3nHNEnOd5+fB6vf6nMaZomoYQQilt25YQ0jSNlJJznjcYYwDAGtv/IKVUSgHANE1FWZYhhHwJABBCeMsJIdZaxphzTgghpbzf75RSpRRjDBEZY+/NlNLj8fj172u32x0Oh7qu53m21ubmAOC9H8dxmqZxHBHRWouI3nsAKKqq2mw26/V6tVptt9uu61JKzrnfwykl732M8fl8IqIxxnuPiMXlchFCnE6nZVn2+/35fBZC3G435xyl1FqbzyaEVFUlpaSUaq1zvoUhIgA8Ho8Yo9Y6hND3vVIqu5FShhCWZYkxImK2k7NAxBijUiobzq1yMWNMzhjjMAxt29Z13XXdOI4pJSHEe3gYhmVZQgj+D7KC/LxsS2stpfwGd2IproYuf4UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MAPPO Representation\"\n        title=\"MAPPO Representation\"\n        src=\"/static/fa3947ed0bdef594d561c880648530c0/39600/mappo-representation.png\"\n        srcset=\"/static/fa3947ed0bdef594d561c880648530c0/1aaec/mappo-representation.png 175w,\n/static/fa3947ed0bdef594d561c880648530c0/98287/mappo-representation.png 350w,\n/static/fa3947ed0bdef594d561c880648530c0/39600/mappo-representation.png 700w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>A representation of MAPPO</em></p>\n<p>By incorporating a mechanism to limit the extent of policy updates, MAPPO ensures that agents make incremental improvements without destabilizing the learned behaviors of other agents. This is particularly important in traffic systems where the actions of one agent can significantly impact others. Through iterative learning and coordination, agents using MAPPO can adapt to dynamic traffic conditions, leading to a more harmonious and efficient traffic management system without the need for centralized control.</p>\n<p>Both MA2C and MAPPO exemplify how MARL algorithms can be tailored to address the complexities of autonomous traffic management. By enabling agents to learn and adapt their strategies in a shared environment, these algorithms contribute to reducing congestion, improving travel times, and enhancing the overall robustness of urban traffic systems.</p>\n<h1>Alternative Approaches</h1>\n<p>The paper <a href=\"https://arxiv.org/html/2408.09675v1#S6\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">\"Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey\" by Ruiqi Zhang et al. (2024)</a> provides a comprehensive overview of the current state of multi-agent reinforcement learning (MARL) in autonomous driving. The paper takes an example scenario where multiple autonomous vehicles are approaching an intersection. Each vehicle has its own goals and constraints, such as reaching its destination quickly while avoiding collisions with other vehicles. The environment is dynamic, with other vehicles and pedestrians entering and exiting the intersection. Using MARL, we can develop a control policy for each vehicle that takes into account the actions of other vehicles and pedestrians. The policy can be trained using a combination of centralized and decentralized training methods.</p>\n<p>The paper introduces two approaches namely Central Training and Decentralized Execution (CTDE) and Decentralized training Decentralized Execution (DTDE). To keep things simple CTDE basically means single policy is trained for all vehicles where as DTDE means individual policies are trained for each vehicle. Advantages of this approaches include but not limited to: Scalability, Partial Observability and Non-stationarity.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/048237396dd95ad7d5d59ba0dab4e451/39600/ctde-dtde.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 114.85714285714286%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,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'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CTDE and DTDE Visualization\"\n        title=\"CTDE and DTDE Visualization\"\n        src=\"/static/048237396dd95ad7d5d59ba0dab4e451/39600/ctde-dtde.png\"\n        srcset=\"/static/048237396dd95ad7d5d59ba0dab4e451/1aaec/ctde-dtde.png 175w,\n/static/048237396dd95ad7d5d59ba0dab4e451/98287/ctde-dtde.png 350w,\n/static/048237396dd95ad7d5d59ba0dab4e451/39600/ctde-dtde.png 700w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>The above figure makes it easy to visualize CTDE and DTDE. (<a href=\"https://arxiv.org/html/2408.09675v1#S6\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ruiqi Zhang et al. (2024)</a>)</em></p>\n<p>Some alternative algorithms that can also be applied to autonomous traffic management, though they may be more complex or condition-dependent are listed below:</p>\n<ol>\n<li><strong>MADDPG (Multi-Agent Deep Deterministic Policy Gradient)</strong></li>\n</ol>\n<p><strong>Multi-Agent Deep Deterministic Policy Gradient (MADDPG)</strong> employs a decentralized approach for each agent while maintaining a shared critic network. In the context of traffic management, each agent — be it a traffic light, a vehicle, or even a pedestrian crossing — learns its policy by interacting with the environment independently. However, the critic network, which evaluates the actions of all agents, helps the system account for the interactions between different agents, leading to coordinated and more efficient decision-making.</p>\n<p>The MADDPG algorithm begins with agents randomly exploring their actions, aided by a noise process to ensure varied exploration. As each agent takes actions, such as adjusting the traffic light duration or accelerating a vehicle, it observes the resulting reward and new state of the system, capturing the complex interdependencies in urban traffic. These experiences are stored in a replay buffer and periodically used to update the actor and critic networks. The actor focuses on selecting the best action based on the local state, while the critic evaluates this action by considering the joint state and actions of all agents, allowing for a better understanding of the cooperative aspects of traffic management. By leveraging these experiences through a combination of policy and value function updates, MADDPG helps agents adapt their behavior to achieve not only their individual goals (like minimizing wait time) but also the global goal of optimizing traffic flow.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 609px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/99603d4cf702462a2f869a77b84a99d3/116b4/maddpg-algorithm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 57.14285714285714%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsSAAALEgHS3X78AAABiUlEQVQoz3WS0Y6cMAxF5/9/rqOquzvSQEIIhGEICcR2IEBCBbtqp1V7Hvx2pHttX0IIbdtcv18ZZ2/vPzhnDpy11pi+050xJqW0/4fLNE3gXM7yLLvfbh85ywrBC8GRcJ6nafLTiT8hIkDw3v+SfYzbo20YZ7zgsiqFFFzwnGWykkrVVSVlJQtRqEadoQwSfsneH7JSddM+jDW96XWvte6IKKW0bVt84e/Y4zjGGJWqM5Z3uivLomkb7wkJHTjyNE2eCAHhU04nX/JnBkSoVd3ppxCFEIXuu1pVoiw63Z2FyTkX45HidX8XxEMGcPfsXtWVrKUoBedcSsl4zgturUWEYRimeTbWruv6W962bd/3GDcAN45WNfX77Y2xvBRClEJWEhFDmIdxAIB1Xf7oHMIcY+z7noj2PYUwOxjpuNMclmUJy7KuISzee0RclvBa+zIMdt9T09RSlsb0drC9OWjbhyzLWlV03DL++0mez2eeZ9+uVwBHhJ8AgLVGa23MMZ1zcIIvpJR+Aic5b4FOi+vZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"MADDPG Algorithm\"\n        title=\"MADDPG Algorithm\"\n        src=\"/static/99603d4cf702462a2f869a77b84a99d3/116b4/maddpg-algorithm.png\"\n        srcset=\"/static/99603d4cf702462a2f869a77b84a99d3/1aaec/maddpg-algorithm.png 175w,\n/static/99603d4cf702462a2f869a77b84a99d3/98287/maddpg-algorithm.png 350w,\n/static/99603d4cf702462a2f869a77b84a99d3/116b4/maddpg-algorithm.png 609w\"\n        sizes=\"(max-width: 609px) 100vw, 609px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>MADDPG Algorithm</em></p>\n<p>One of the distinguishing features of MADDPG is its use of centralized training and decentralized execution. During training, the critic utilizes information from all agents, providing a more holistic view of the entire traffic network. However, during real-time execution, each agent makes decisions independently based on its learned policy. This balance of centralized and decentralized strategies makes MADDPG well-suited for highly dynamic traffic systems, where real-time coordination is crucial but direct communication between all agents may not be feasible. As agents continue to learn and refine their policies, MADDPG contributes to a coordinated approach to traffic management, reducing congestion and promoting smoother traffic movement across the system.</p>\n<p><strong>2. Independent Advantageous Actor-Critic (IA2C) Algorithm for MARL</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ab8abbf4e1cf78538ab4ca2d6e8bf206/39600/ia2c-algorithm.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 102.28571428571429%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAABw0lEQVQ4y5WS2ZKaQBhGff/3mmiGRQgCjYogNItA0yuNbaOmZkyqcqOSc3/q+5dvgTG2LAsAEHwzjuP9L7fb7f6ShRACAGDbtud5SZp0XRfH8TbaFkVBCHkjn89nAMByuXQcZ7/fQwjruq6qSgjxPllrnef59hvHcY7HI2NsHMdpmu7vWHRdZxjGarWK4zhN02EY5mz7RyaEuK5rmiYAIAzDPM//Q+77/nQ6CSEYY7jHjLH7bBYIoQhEjuM8vkUpxRgrpWbJnPMgCGzbdh3X87w0TUEIBvG1+fV6vX3zSkYIfZm/vDAM/Y1vWRYhRGv9/lVd10kpsyz7/PlpmubHjw/XdQEAURQ1TVNV1YsrLB4h4zhuNhsIYQELCGGe523b9n2vlNJaP5WbpgmCoK7rvu8xxpxzSukwDFLKf3v+NFkOknPub/woigpYZMcMIcQ5l1JSSi+Xy1O5bVvOudZ6t9tlWVbXtZSSMUYpVUpJKV+NXZYlxniapjRNm6YpiiLLsrkloZRqraWUcRw/zMPhcL1eZ8kQQt/3lVIY48eFhRBzk9u2HccRIbRerwEASZIghObKhJCyLA3DSA7Jo5Ja62kevwHBk3mcpIxzvQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"IA2C Algorithm\"\n        title=\"IA2C Algorithm\"\n        src=\"/static/ab8abbf4e1cf78538ab4ca2d6e8bf206/39600/ia2c-algorithm.png\"\n        srcset=\"/static/ab8abbf4e1cf78538ab4ca2d6e8bf206/1aaec/ia2c-algorithm.png 175w,\n/static/ab8abbf4e1cf78538ab4ca2d6e8bf206/98287/ia2c-algorithm.png 350w,\n/static/ab8abbf4e1cf78538ab4ca2d6e8bf206/39600/ia2c-algorithm.png 700w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p><em>Independent Advantage Actor Critic Algorithm</em></p>\n<p>This algorithm is an extension of the actor-critic method, where each agent learns independently but still benefits from the feedback of a critic to improve decision-making.</p>\n<p>In the Independent Advantageous Actor-Critic (IA2C) algorithm, the process begins by initializing key parameters, including the actor's policy parameters θaθa and the critic's value function parameters ψaψa, along with an entropy term to promote exploration. In this project, <strong>Simulation of Urban Mobility (SUMO)</strong> is used to create a realistic traffic environment where each vehicle, acting as an independent agent, navigates through traffic with the goal of optimizing flow and reducing congestion. Each vehicle learns independently, which means it doesn't rely on centralized coordination and can adapt to its own local environment, allowing the system to scale effectively.</p>\n<p>During training, a memory buffer stores each agent's experiences, which consist of the actions they take, the states they observe, and the rewards they receive. At every time step, the agents use their current policy to select an action based on the state of the environment. After performing the action, the agent observes the new state and receives feedback in the form of a reward, which indicates how well the action reduced traffic congestion or achieved other desired outcomes.</p>\n<p>The critic then steps in to evaluate the action by calculating the <strong>Temporal Difference (TD) error</strong>, which measures the difference between the predicted reward and the actual reward the agent received. The critic's role is crucial, as it provides feedback to the actor on how well the actions are aligned with long-term rewards. To encourage exploration, the algorithm uses an entropy term. This ensures that agents do not get stuck in a local optimum by always choosing the same actions, but instead explore a variety of strategies to find more effective solutions.</p>\n<p>The actor's loss is then computed based on how well its chosen action maximized rewards, which is used to update its policy parameters θaθa for better future decision-making. Simultaneously, the critic's loss is calculated by comparing the TD error, and the critic's parameters ψaψa are updated to improve its estimation of future rewards. This dual update of both actor and critic parameters enables each agent to continuously refine its strategies.</p>\n<p>Over multiple episodes, this iterative learning process allows the agents to converge toward optimal policies. By interacting with the dynamic traffic environment, observing outcomes, and adjusting their actions, the agents eventually learn effective strategies for managing traffic autonomously, improving flow, and reducing congestion over time. This decentralized approach, using IA2C, is particularly effective for real-time, large-scale environments like traffic management.</p>\n<h1>Summary</h1>\n<p>In conclusion, Multi-Agent Reinforcement Learning (MARL) offers a promising approach to managing the complexities of autonomous traffic systems, transforming static and rigid traffic control into a dynamic and adaptive system. By allowing multiple agents, such as vehicles and traffic lights, to interact and learn from their environment in a decentralized yet cooperative manner, MARL algorithms like MA2C, MAPPO, and MADDPG are paving the way for more efficient, resilient, and scalable urban traffic management solutions. Through the application of advanced algorithms and cooperative strategies, MARL has the potential to significantly reduce congestion, improve travel times, and create a safer and more responsive urban mobility landscape.</p>\n<p>I would like to thank Suraj Kalwaghe and Tanmay Pancholi for contributing in writing of this article.</p>\n<h1>References</h1>\n<ol>\n<li>\n<p>M. Brittain and P. Wei, \"Autonomous separation assurance in an high density enroute sector: A deep multi-agent reinforcement learning approach,\" in 2019 IEEE Intelligent transportation Systems Conference (ITSC), pp. 3256–3262, 2019.</p>\n</li>\n<li>\n<p>J. Yang, J. Zhang, and H. Wang, \"Urban traffic control in software defined internet of things via a multi-agent deep reinforcement learning approach,\" IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 6, pp. 3742–3754, 2021.</p>\n</li>\n<li>\n<p>Ruiqi Zhang, Jing Hou, Florian Walter, Shangding Gu, Jiayi Guan, Florian Röhrbein, Yali Du, Panpan Cai, Guang Chen, Alois Knoll (2024). <em>Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey</em>. arXiv.</p>\n</li>\n<li>\n<p>Yang, Y., Hao, J., Zheng, Y., &#x26; Cai, H. (2023). <em>Multi-agent reinforcement learning: Foundations and Modern Approaches.</em></p>\n</li>\n</ol>","frontmatter":{"title":"Multi-Agent RL Approach to Traffic Management","description":"Exploring how multi-agent reinforcement learning can revolutionize traffic management systems","date":"2024-10-15","slug":"/blog/multi-agent-rl-traffic-management","tags":["Reinforcement Learning","Traffic Management","Machine Learning","AI"]}}},"pageContext":{}},
    "staticQueryHashes": ["1994492073","2009693873","3825832676","4162897811"]}